{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":85892,"databundleVersionId":9721658,"sourceType":"competition"},{"sourceId":11957,"sourceType":"datasetVersion","datasetId":8542},{"sourceId":1246668,"sourceType":"datasetVersion","datasetId":715814}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-11T04:55:09.780950Z","iopub.execute_input":"2024-10-11T04:55:09.781424Z","iopub.status.idle":"2024-10-11T04:55:10.182065Z","shell.execute_reply.started":"2024-10-11T04:55:09.781336Z","shell.execute_reply":"2024-10-11T04:55:10.181159Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/intelligence-sig-NLP-Task/sample_submission.csv\n/kaggle/input/intelligence-sig-NLP-Task/news_train.csv\n/kaggle/input/intelligence-sig-NLP-Task/test.csv\n/kaggle/input/glove6b100dtxt/glove.6B.100d.txt\n/kaggle/input/glove6b50dtxt/glove.6B.50d.txt\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ndata = pd.read_csv(\"/kaggle/input/intelligence-sig-NLP-Task/news_train.csv\")\nx = data.drop('Category',axis=1)\ny = data['Category']\n\n# Splitting data into training and testing pools with 80 20 split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2) ","metadata":{"execution":{"iopub.status.busy":"2024-10-11T04:55:10.183645Z","iopub.execute_input":"2024-10-11T04:55:10.184035Z","iopub.status.idle":"2024-10-11T04:55:10.847804Z","shell.execute_reply.started":"2024-10-11T04:55:10.184001Z","shell.execute_reply":"2024-10-11T04:55:10.846984Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## **First Approach**\nUsing Bag of Words and Naive Bayes determine the category. This helps us set a baseline scores for all metrics.\n\n\n#### Why I decided to use Bag of Words?\n* Bag of words is useful when the text pool is really small, here the the fields consisted of mainly short sentences which were really decriptive and concise.\n* It is really simple to implement and works well with Naive Bayes.\n* Since use of pre trained models were not allowed, the inofrmation the model learn only depended on the information available in the tarining data.\n\n#### Why i decided to include stop words.\n* The only category which was concerning was the humour category which includes \"keywords\"  which if stopwords were ommited could consist of embedding mainly composed of these \"keywords \" which were prevelant in other categories and confise the model. In certain examples I observed that the data in this category were in terms of questions, so I decided to keep in stopwords, in hopes that it will be able to capture this.","metadata":{}},{"cell_type":"markdown","source":"#### How Bag of Words works\nFor each category there exists a dictionary such that each dictionry consists of frequencies of words that occur in that category independent of all the occurances of said word in other categories. To start of tokenisation, I have joined the head and the title into one sentence and used that to iterate over every word in in that sentence and added and/or updated the frequency of the occured word. Also removed any special characters that occur for simplification.\n\nNow there is the use of lemmitisation or stemming that can be used here for further improving results during tokensation, since this is a benchmark trial I made decision to skip this and include it further improvements.","metadata":{}},{"cell_type":"code","source":"print(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-11T04:55:10.848914Z","iopub.execute_input":"2024-10-11T04:55:10.849281Z","iopub.status.idle":"2024-10-11T04:55:10.859530Z","shell.execute_reply.started":"2024-10-11T04:55:10.849240Z","shell.execute_reply":"2024-10-11T04:55:10.858227Z"}},"outputs":[{"name":"stdout","text":"          ID                                         News_title  \\\n0          1  Do men enjoy sex more, or women? The Mahabhara...   \n1          2       Why you should eat the Demonetisation laddoo   \n2          3            Is the world headed for a new Cold War?   \n3          4  Demonetisation is all about Modi, either you'r...   \n4          5  Why electoral bonds won't clean up political f...   \n...      ...                                                ...   \n15571  15572  [Watch] Hansal Mehta beautifully captures Rohi...   \n15572  15573  How Pakistan weakened Taliban by revealing Mul...   \n15573  15574          2G 'scam': What the verdict means for DMK   \n15574  15575  Kargil to Pathankot: Why is India never prepar...   \n15575  15576      On Sadhvis and sex workers: Listen up, trolls   \n\n                                           News_headline  Category  \n0      [Book Extract] From Anushasana Parva, translat...      Arts  \n1      One laddoo equals to one lakh in your Jan Dhan...    humour  \n2      The battle lines have become very clear with R...  politics  \n3      How many times should this hypocritical drama ...  business  \n4      Union finance minister Arun Jaitley needs to p...  politics  \n...                                                  ...       ...  \n15571  'The Last Letter: Reach for the Stars' was sho...  politics  \n15572  This is essential for getting the support of t...  politics  \n15573  The real story will be in how DMK working pres...  politics  \n15574  It appears that history is constantly repeatin...  politics  \n15575  Abuse is part of the parlance of the troll Naz...  politics  \n\n[15576 rows x 4 columns]\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import re\n\n# Dictionary to store bag of words for each category\nbag_of_words = {\n    'tech': {},\n    'politics': {},\n    'Arts': {},\n    'humour': {},\n    'sports': {},\n    'business': {}\n}\npattern = re.compile(r'[^A-Za-z0-9\\s]')\nfor r in range(y_train.shape[0]):\n    category = y_train.iloc[r]\n    title = x_train.iloc[r, 1]\n    head = x_train.iloc[r, 2]\n    words = f\"{head} {title}\" # Combining both the title and the head into one string\n    for word in words.split():\n        key = pattern.sub('', word).lower() # Remove special characters and convert to lowercase\n        bag_of_words[category][key] = bag_of_words[category].get(key, 0) + 1\n\n# print(bag_of_words_sports)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-11T04:55:10.862276Z","iopub.execute_input":"2024-10-11T04:55:10.862618Z","iopub.status.idle":"2024-10-11T04:55:12.259277Z","shell.execute_reply.started":"2024-10-11T04:55:10.862583Z","shell.execute_reply":"2024-10-11T04:55:12.258385Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"#### Naive Bayes\nNow that we have the bag of words for every class we can calculate the probability of class using Bayes Rules,\n\n<center> $P(Class \\mid Words) = \\frac{P(Words \\mid Class) \\cdot P(Class)}{P(Words)}$ <center>\n\n\n\nWe omit $P(Words)$ as the value which maximises $\\frac{P(Words \\mid Class) \\cdot P(Class)}{P(Words)}$ also maximises, $P(Words \\mid Class) \\cdot P(Class)$\n\nWe assume the words occur independent of each other therefore $P(Words \\mid Class) = \\prod_{i} P(Word_i \\mid Class)$ \n\nProduct of probabilities can get very small so we add the logs of the proabilities, \n<center> $\\log P(Words \\mid Class) = \\sum_{i} \\log P(Word_i \\mid Class)$ <center>\n\nProbability of class is given by,\n\n$P(C) = \\frac{Number\\space of\\space training\\space entires\\space for\\space a\\space given\\space class}{Total\\space number\\space of\\space training\\space entries}$","metadata":{}},{"cell_type":"code","source":"counts = y_train.value_counts() # Library of class occurances in training entry\ntotal_entries = y_train.shape[0] # Total number of entires\n\nprint(counts)\nprint(\"Total entries = \",total_entries)","metadata":{"execution":{"iopub.status.busy":"2024-10-11T04:55:12.260782Z","iopub.execute_input":"2024-10-11T04:55:12.261177Z","iopub.status.idle":"2024-10-11T04:55:12.273715Z","shell.execute_reply.started":"2024-10-11T04:55:12.261129Z","shell.execute_reply":"2024-10-11T04:55:12.272664Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Category\npolitics    9225\nsports       733\nhumour       731\nArts         672\ntech         574\nbusiness     525\nName: count, dtype: int64\nTotal entries =  12460\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"y_pred = []\npattern = re.compile(r'[^A-Za-z0-9\\s]')\npreds = { \n    'tech': 0,\n    'politics': 0,\n    'Arts': 0,\n    'humour': 0,\n    'sports': 0,\n    'business': 0\n}\n\nfor r in range(y_test.shape[0]):\n    correct_category = y_test.iloc[r]\n    title = x_test.iloc[r, 1]\n    head = x_test.iloc[r, 2]\n    words = f\"{head} {title}\"\n    for category in bag_of_words:\n        p_c = counts[category] / total_entries\n        log_sum = np.log(p_c)\n        bag = bag_of_words[category]\n        for word in words.split():\n            key = pattern.sub('', word).lower()\n            if key in bag:\n                n_word = bag[key]\n                n_total = sum(bag.values())\n                p_word = n_word / n_total\n            else:\n                p_word = 1e-6  # Assigning a very small value for unknown words since we take log\n\n            log_sum += np.log(p_word)\n\n        preds[category] = log_sum\n    max_key = max(preds, key=preds.get)\n    y_pred.append(max_key)\n\n# print(y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-10-11T04:55:12.274974Z","iopub.execute_input":"2024-10-11T04:55:12.275358Z","iopub.status.idle":"2024-10-11T04:55:56.722804Z","shell.execute_reply.started":"2024-10-11T04:55:12.275311Z","shell.execute_reply":"2024-10-11T04:55:56.721910Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from sklearn.metrics import f1_score\n\nf1_score = f1_score(y_test, y_pred, average = 'weighted')\nprint(f'F1 score: {f1_score}')","metadata":{"execution":{"iopub.status.busy":"2024-10-11T04:56:19.330922Z","iopub.execute_input":"2024-10-11T04:56:19.332226Z","iopub.status.idle":"2024-10-11T04:56:19.364392Z","shell.execute_reply.started":"2024-10-11T04:56:19.332174Z","shell.execute_reply":"2024-10-11T04:56:19.363411Z"},"trusted":true},"outputs":[{"name":"stdout","text":"F1 score: 0.8448133709909017\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/intelligence-sig-NLP-Task/news_train.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-10-11T04:56:21.739886Z","iopub.execute_input":"2024-10-11T04:56:21.740524Z","iopub.status.idle":"2024-10-11T04:56:21.800880Z","shell.execute_reply.started":"2024-10-11T04:56:21.740483Z","shell.execute_reply":"2024-10-11T04:56:21.799990Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"y_pred = []\npattern = re.compile(r'[^A-Za-z0-9\\s]')\npreds = { \n    'tech': 0,\n    'politics': 0,\n    'Arts': 0,\n    'humour': 0,\n    'sports': 0,\n    'business': 0\n}\n\nfor r in range(test.shape[0]):\n    title = test.iloc[r, 1]\n    head = test.iloc[r, 2]\n    words = f\"{head} {title}\"\n    for category in bag_of_words:\n        p_c = counts[category] / total_entries\n        log_sum = np.log(p_c)\n        bag = bag_of_words[category]\n        for word in words.split():\n            key = pattern.sub('', word).lower()\n            if key in bag:\n                n_word = bag[key]\n                n_total = sum(bag.values())\n                p_word = n_word / n_total\n            else:\n                p_word = 1e-6  # Assigning a small value for unknown words since we take log\n\n            log_sum += np.log(p_word)\n\n        preds[category] = log_sum\n    max_key = max(preds, key=preds.get)\n    y_pred.append(max_key)\n\n# print(y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-10-11T04:56:49.430169Z","iopub.execute_input":"2024-10-11T04:56:49.430575Z","iopub.status.idle":"2024-10-11T05:00:34.116185Z","shell.execute_reply.started":"2024-10-11T04:56:49.430535Z","shell.execute_reply":"2024-10-11T05:00:34.115211Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Making the submission\nidx = test.iloc[:,0]\nmapping = {\n    \"Arts\" : 0,\n    \"business\" : 1,\n    \"humour\" : 2,\n    \"politics\" : 3,\n    \"sports\" : 4,\n    \"tech\" : 5\n}\ny_pred = [mapping.get(category, category) for category in y_pred]\nout = pd.DataFrame({'ID':idx, 'Category': y_pred})\nout.to_csv('pred1.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-11T04:56:44.136551Z","iopub.status.idle":"2024-10-11T04:56:44.136923Z","shell.execute_reply.started":"2024-10-11T04:56:44.136745Z","shell.execute_reply":"2024-10-11T04:56:44.136763Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### ****Submission gave an F1 Score of 0.84574 which is going to be the baseline to beat****","metadata":{}},{"cell_type":"markdown","source":"#### Pros\n1. Model is efficient and fast.\n2. Ignore irrelevant features.\n3. Handles proper nouns or class specific words well.\n\n#### Cons\n1. Does not have any kind of contextual understanding or attention applied.\n\n#### Things I can use to improve my score,\n1. Stemming or Lemmitization.\n2. Avoiding stop words.\n3. Better classifying techniques.\n4. Handling context better.\n\n#### Concerns\nMy main concern is handling proper nouns, like for exmaple,\n\n*\tSentence 1, “Cristiano Ronaldo is making headlines for his recent performance.”\n*\tSentence 2, “Sensex is making headlines for its recent performance.”\n\nIf we choose to replace all proper nouns with a masking vector, both the sentences will be meaning the same, and the context only chnges when the sentences have the specifc names in them.\nSentence 1, can be categorised under sports, as the main subject is Cristiano Ronaldo and Sentence 2, can be categorised under business, the main subject being Sensex","metadata":{}},{"cell_type":"markdown","source":"## Second Approach\n#### Moving from Bag of Wrods to TF-IDF\nUsing TF-IDF (Term Frequency Inverse Document Frequency), this method is an upgrade in most cases as TF-IDF adjusts for word importance by considering how often a word appears across different categories. It emphasizes on words that are unique to a particular category, making it more suitable for capturing the features of each category.\n\n#### Moving from Naive Bayes to Logistic Regression\nNaive Bayes assumes independence of occurance of terms pertaining to features unique to each category, Logistic Regression directly predcits the probability of categories using a logistic function, which may provide better probability estimates.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ndata = pd.read_csv(\"/kaggle/input/intelligence-sig-NLP-Task/news_train.csv\")\nx = data.drop('Category',axis=1)\ny = data['Category']\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2024-10-11T05:00:34.117806Z","iopub.execute_input":"2024-10-11T05:00:34.118155Z","iopub.status.idle":"2024-10-11T05:00:34.184903Z","shell.execute_reply.started":"2024-10-11T05:00:34.118114Z","shell.execute_reply":"2024-10-11T05:00:34.183929Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"#### **Formula for traditional TF-IDF**\n\n$ TF(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d} $\n\n$ IDF(t) = \\log \\left(\\frac{\\text{Total number of documents}}{\\text{Number of documents containing term } t} \\right) $\n\nThe final TF-IDF score for a term in a document is the product of TF and IDF:\n\n$ TF\\text{-}IDF(t, d) = TF(t, d) \\times IDF(t) $\n\n#### Vectorization in TF-IDF\nOnce the TF-IDF is obtained score for every word, the sentence can be represented as a vector according to the TF-IDF score of words in it. Each sentence becomes a vector in a multi-dimensional space.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\n\ntfidf = TfidfVectorizer(max_features=50000, stop_words='english')\n\n# print(x_train)\nx_train['News_title'] = x_train['News_title'].fillna(' ')\nx_train['News_headline'] = x_train['News_headline'].fillna(' ')\nx_test['News_title'] = x_test['News_title'].fillna(' ')\nx_test['News_headline'] = x_test['News_headline'].fillna(' ')\nx_train_tfidf = tfidf.fit_transform(x_train[\"News_title\"]+\" \"+x_train[\"News_headline\"])\nx_test_tfidf = tfidf.transform(x_test[\"News_title\"]+\" \"+x_test[\"News_headline\"])\n\n# Using Logistic Regression\nmodel = LogisticRegression(max_iter=1000)\n\n# print(x_train_tfidf)\nmodel.fit(x_train_tfidf, y_train)\n\ny_pred = model.predict(x_test_tfidf)\n\nf1_score = f1_score(y_test, y_pred, average = 'weighted')\nprint(f'F1 score: {f1_Score}')\n\nprint(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-10-11T04:55:57.168390Z","iopub.status.idle":"2024-10-11T04:55:57.168802Z","shell.execute_reply.started":"2024-10-11T04:55:57.168623Z","shell.execute_reply":"2024-10-11T04:55:57.168641Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom nltk.stem import PorterStemmer\nimport nltk\n\nnltk.download('punkt') \nstemmer = PorterStemmer() # Initializing stemmer for stemming\n\ndef stem_text(text):\n    tokens = nltk.word_tokenize(text)\n    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n    return ' '.join(stemmed_tokens)\n\nx_train['News_title'] = x_train['News_title'].fillna(' ')\nx_train['News_headline'] = x_train['News_headline'].fillna(' ')\nx_test['News_title'] = x_test['News_title'].fillna(' ')\nx_test['News_headline'] = x_test['News_headline'].fillna(' ')\n\nx_train['Processed_Text'] = x_train[\"News_title\"] + \" \" + x_train[\"News_headline\"]\nx_train['Processed_Text'] = x_train['Processed_Text'].apply(stem_text)\n\nx_test['Processed_Text'] = x_test[\"News_title\"] + \" \" + x_test[\"News_headline\"]\nx_test['Processed_Text'] = x_test['Processed_Text'].apply(stem_text)\n\ntfidf = TfidfVectorizer(max_features=50000, stop_words='english')\n\nx_train_tfidf = tfidf.fit_transform(x_train['Processed_Text'])\nx_test_tfidf = tfidf.transform(x_test['Processed_Text'])","metadata":{"execution":{"iopub.status.busy":"2024-10-11T04:55:57.170069Z","iopub.status.idle":"2024-10-11T04:55:57.170394Z","shell.execute_reply.started":"2024-10-11T04:55:57.170229Z","shell.execute_reply":"2024-10-11T04:55:57.170246Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = LogisticRegression(max_iter=1000)\nmodel.fit(x_train_tfidf, y_train)\n\ny_pred = model.predict(x_test_tfidf)\n\nprint(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-10-11T04:55:57.171615Z","iopub.status.idle":"2024-10-11T04:55:57.171945Z","shell.execute_reply.started":"2024-10-11T04:55:57.171779Z","shell.execute_reply":"2024-10-11T04:55:57.171797Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Swicthing from Logistic Regression to SVMs\n\nUsually SVMs are used for Binary classification, sometimes they work well for mutliclass problems as well. SVMS are higly customziable depending on the kernel, regularisation, class imbalance and other tunable parameters that it offers.","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\n\nsvm_model = SVC()\n\nsvm_model.fit(x_train_tfidf, y_train)\n\ny_pred_svm = svm_model.predict(x_test_tfidf)\nprint(classification_report(y_test, y_pred_svm))","metadata":{"execution":{"iopub.status.busy":"2024-10-11T04:55:57.173265Z","iopub.status.idle":"2024-10-11T04:55:57.173666Z","shell.execute_reply.started":"2024-10-11T04:55:57.173457Z","shell.execute_reply":"2024-10-11T04:55:57.173495Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Experimenting with pre trained tokenizers and vectorization.","metadata":{}},{"cell_type":"code","source":"pip install keras","metadata":{"execution":{"iopub.status.busy":"2024-10-11T04:55:57.174898Z","iopub.status.idle":"2024-10-11T04:55:57.175248Z","shell.execute_reply.started":"2024-10-11T04:55:57.175075Z","shell.execute_reply":"2024-10-11T04:55:57.175093Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import word_tokenize\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, GlobalMaxPool1D\nfrom tensorflow.keras.utils import to_categorical\n\nnltk.download('punkt')\n\ndf = pd.read_csv('/kaggle/input/intelligence-sig-NLP-Task/news_train.csv')\ndf['News_title'] = df['News_title'].fillna(' ')\ndf['News_headline'] = df['News_headline'].fillna(' ')\n\ndf['Text'] = df['News_title'] + \" \" + df['News_headline']\n\nle = LabelEncoder()\ndf['target'] = le.fit_transform(df['Category'])\nX_train, X_test, y_train, y_test = train_test_split(df['Text'], df['target'], test_size=0.2, random_state=42)\n\ntokenizer = Tokenizer(num_words=50000, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(X_train)\nX_train_seq = tokenizer.texts_to_sequences(X_train)\nX_test_seq = tokenizer.texts_to_sequences(X_test)\n\nX_train_pad = pad_sequences(X_train_seq, maxlen=100, padding='post')\nX_test_pad = pad_sequences(X_test_seq, maxlen=100, padding='post')\ny_train_cat = to_categorical(y_train)\ny_test_cat = to_categorical(y_test)","metadata":{"execution":{"iopub.status.busy":"2024-10-11T04:55:57.176234Z","iopub.status.idle":"2024-10-11T04:55:57.176623Z","shell.execute_reply.started":"2024-10-11T04:55:57.176399Z","shell.execute_reply":"2024-10-11T04:55:57.176424Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embedding_index = {}\nglove_path = '/kaggle/input/glove6b100dtxt/glove.6B.100d.txt'\n\nwith open(glove_path, encoding=\"utf-8\") as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embedding_index[word] = coefs\n\nvocab_size = len(tokenizer.word_index) + 1\nembedding_dim = 100  # GloVe 100d\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\n\nfor word, index in tokenizer.word_index.items():\n    embedding_vector = embedding_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector","metadata":{"execution":{"iopub.status.busy":"2024-10-11T04:55:57.178159Z","iopub.status.idle":"2024-10-11T04:55:57.178530Z","shell.execute_reply.started":"2024-10-11T04:55:57.178327Z","shell.execute_reply":"2024-10-11T04:55:57.178344Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from keras.layers import Bidirectional\n\nmodel = Sequential()\nmodel.add(Embedding(input_dim=vocab_size, \n                    output_dim=embedding_dim, \n                    weights=[embedding_matrix], \n                    input_length=100, \n                    trainable=False))\nmodel.add(Bidirectional(LSTM(128, return_sequences=True)))\nmodel.add(GlobalMaxPool1D())\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(6, activation='softmax'))\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model.fit(X_train_pad, y_train_cat, epochs=10, batch_size=32, validation_data=(X_test_pad, y_test_cat))","metadata":{"execution":{"iopub.status.busy":"2024-10-11T04:55:57.182458Z","iopub.status.idle":"2024-10-11T04:55:57.182939Z","shell.execute_reply.started":"2024-10-11T04:55:57.182697Z","shell.execute_reply":"2024-10-11T04:55:57.182722Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"loss, accuracy = model.evaluate(X_test_pad, y_test_cat)\nprint(f'Test Accuracy: {accuracy * 100:.2f}%')\n\ny_pred = model.predict(X_test_pad)\ny_pred_classes = np.argmax(y_pred, axis=1)\n\ny_test_labels = le.inverse_transform(np.argmax(y_test_cat, axis=1))\ny_pred_labels = le.inverse_transform(y_pred_classes)\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test_labels, y_pred_labels))","metadata":{"execution":{"iopub.status.busy":"2024-10-11T04:55:57.183997Z","iopub.status.idle":"2024-10-11T04:55:57.184464Z","shell.execute_reply.started":"2024-10-11T04:55:57.184213Z","shell.execute_reply":"2024-10-11T04:55:57.184236Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test = pd.read_csv(\"/kaggle/input/intelligence-sig-NLP-Task/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-10-11T04:55:57.186449Z","iopub.status.idle":"2024-10-11T04:55:57.186927Z","shell.execute_reply.started":"2024-10-11T04:55:57.186684Z","shell.execute_reply":"2024-10-11T04:55:57.186709Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test['News_title'] = df['News_title'].fillna(' ')\ndf_test['News_headline'] = df['News_headline'].fillna(' ')\n\ndf_test['Text'] = df_test['News_title'] + \" \" + df_test['News_headline']\n\ntokenizer = Tokenizer(num_words=50000, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(X_train)\ntest_seq = tokenizer.texts_to_sequences(df_test['Text'])\n\ntest_pad = pad_sequences(test_seq, maxlen=100, padding='post')\n\ny_pred = model.predict(test_pad)\nprint(f'Test Accuracy: {accuracy * 100:.2f}%')\ny_pred_classes = np.argmax(y_pred, axis=1)\n\ny_pred_labels = le.inverse_transform(y_pred_classes)","metadata":{"execution":{"iopub.status.busy":"2024-10-11T04:55:57.188159Z","iopub.status.idle":"2024-10-11T04:55:57.188657Z","shell.execute_reply.started":"2024-10-11T04:55:57.188383Z","shell.execute_reply":"2024-10-11T04:55:57.188407Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Length of df_test: {len(df_test)}\")\nprint(f\"Length of test_seq: {len(test_pad)}\")\nprint(f\"Length of y_pred_labels: {len(y_pred_labels)}\")\nidx = df_test['ID']\n\nmapping = {\n    \"Arts\": 0,\n    \"business\": 1,\n    \"humour\": 2,\n    \"politics\": 3,\n    \"sports\": 4,\n    \"tech\": 5\n}\n\nprint(f\"Number of predictions: {len(y_pred_labels)}\")\n\ny_pred_mapped = [mapping.get(category, category) for category in y_pred_labels]\n\nout = pd.DataFrame({'ID': idx, 'Category': y_pred_mapped})\n\nout.to_csv('pred2.1.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-11T04:55:57.190185Z","iopub.status.idle":"2024-10-11T04:55:57.190584Z","shell.execute_reply.started":"2024-10-11T04:55:57.190367Z","shell.execute_reply":"2024-10-11T04:55:57.190386Z"},"trusted":true},"outputs":[],"execution_count":null}]}